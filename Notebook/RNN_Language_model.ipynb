{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYyx4eMZ3nwRlnB7LnvbZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hjhirp/RNN-Language-Model-Implementation/blob/main/Notebook/RNN_Language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QXgQFkp88eE8",
        "outputId": "f5b433fe-ffb7-44df-892e-a6ece2e03aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  Jack and Jill went up the hill .\n",
            " To fetch a pail of water .\n",
            " Jack fell down and broke his crown .\n",
            " And Jill came tumbling after . <class 'str'>\n",
            "Data_Splitted: [' Jack and Jill went up the hill .', ' To fetch a pail of water .', ' Jack fell down and broke his crown .', ' And Jill came tumbling after .'] <class 'list'>\n",
            "Word Indices: {'.': 1, 'and': 2, 'jack': 3, 'jill': 4, 'went': 5, 'up': 6, 'the': 7, 'hill': 8, 'to': 9, 'fetch': 10, 'a': 11, 'pail': 12, 'of': 13, 'water': 14, 'fell': 15, 'down': 16, 'broke': 17, 'his': 18, 'crown': 19, 'came': 20, 'tumbling': 21, 'after': 22}\n",
            "Vocab Size: 23\n",
            "Sequences: [[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]] <class 'list'> 4\n",
            "X= [[3, 2, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14], [3, 15, 16, 2, 17, 18, 19], [2, 4, 20, 21, 22]] y= [[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]] <class 'list'> <class 'list'>\n",
            "Maxlen: 7\n",
            "X: [[ 0  3  2  4  5  6  7  8]\n",
            " [ 0  0  9 10 11 12 13 14]\n",
            " [ 0  3 15 16  2 17 18 19]\n",
            " [ 0  0  0  2  4 20 21 22]] <class 'numpy.ndarray'> (4, 8)\n",
            "y: [[ 3  2  4  5  6  7  8  1]\n",
            " [ 0  9 10 11 12 13 14  1]\n",
            " [ 3 15 16  2 17 18 19  1]\n",
            " [ 0  0  2  4 20 21 22  1]] <class 'numpy.ndarray'> (4, 8)\n",
            "X= [[ 0  3  2  4  5  6  7  8]\n",
            " [ 0  0  9 10 11 12 13 14]\n",
            " [ 0  3 15 16  2 17 18 19]\n",
            " [ 0  0  0  2  4 20 21 22]] y= [[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]] <class 'numpy.ndarray'> <class 'numpy.ndarray'> (4, 8) (4, 8, 23)\n",
            "(None, None, 10)\n",
            "(None, None, 50)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 10)          230       \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, None, 50)          3050      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 23)          1173      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,453\n",
            "Trainable params: 4,453\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input Shape of all Layers: (None, None) (None, None, 10) (None, None, 50)\n",
            "Input Dim: 23\n",
            "-----------------------sample all seq without seed-----------------------------\n",
            "i: 0 Encoded: []\n",
            "i: 0 Encoded: [[0]] (1, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5cb1b1bc1588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------sample all seq without seed-----------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_all_seq_wo_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5cb1b1bc1588>\u001b[0m in \u001b[0;36msample_all_seq_wo_seed\u001b[0;34m(model, tokenizer, n_words, vocab_size)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# predict probability and sample a word from vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Prob:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_proba'"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import numpy\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "data = \"\"\" Jack and Jill went up the hill .\\n To fetch a pail of water .\\n Jack fell down and broke his crown .\\n And Jill came tumbling after .\"\"\"\n",
        "print(\"Data:\", data, type(data))\n",
        "\n",
        "data_splitted=data.split('\\n') #returns a list of strings\n",
        "print(\"Data_Splitted:\", data_splitted, type(data_splitted))\n",
        "\n",
        "tokenizer=Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~')\n",
        "\n",
        "tokenizer.fit_on_texts(data_splitted) #learns a vocabulary\n",
        "print(\"Word Indices:\", tokenizer.word_index) #tokenizer.word_index is a dictionary\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "\n",
        "sequences=tokenizer.texts_to_sequences(data_splitted) #list of list\n",
        "l=len(sequences)\n",
        "print(\"Sequences:\",sequences, type(sequences), l)\n",
        "\n",
        "X=list()\n",
        "y=list()\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  X.insert(i,sequences[i][:-1])\n",
        "  y.insert(i,sequences[i])\n",
        "\n",
        "print(\"X=\", X, \"y=\",y, type(X), type(y))\n",
        "\n",
        "maxlen = max([len(sequence) for sequence in X])\n",
        "print(\"Maxlen:\",maxlen)\n",
        "\n",
        "#X=array(X)\n",
        "#y=array(y)\n",
        "#X=numpy.reshape(X,newshape=(l,-1))\n",
        "#y=numpy.reshape(y,newshape=(l,-1))\n",
        "\n",
        "X=pad_sequences(X,maxlen=maxlen+1,padding='pre') # +1 to have 0 as the first input\n",
        "print(\"X:\",X, type(X), X.shape)\n",
        "y=pad_sequences(y,maxlen=maxlen+1,padding='pre')\n",
        "print(\"y:\",y, type(y), y.shape)\n",
        "\n",
        "#z=numpy.\n",
        "#for i in range(l):\n",
        "#  z[i]=to_categorical(y[i],num_classes=vocab_size)\n",
        "\n",
        "#print(\"X=\", X, \"y=\",y)\n",
        "y=to_categorical(y,num_classes=vocab_size)\n",
        "\n",
        "print(\"X=\", X, \"y=\",y, type(X), type(y), X.shape, y.shape)\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10))\n",
        "print(model.output_shape)\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "print(model.output_shape)\n",
        "model.add(Dense(units=vocab_size,activation='softmax'))\n",
        "model.summary()\n",
        "print(\"Input Shape of all Layers:\",model.layers[0].input_shape,model.layers[1].input_shape,model.layers[2].input_shape)\n",
        "print(\"Input Dim:\",model.layers[0].input_dim)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "'''def sample_seq_wo_seed(model, tokenizer, n_words, vocab_size): #only the first word is sampled\n",
        "  encoded=list()\n",
        "  in_text = ''\n",
        "  # generate a fixed number of words = n_words\n",
        "  for i in range(n_words):\n",
        "    # encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0] # for words not in the vocab it returns []\n",
        "    # pre-pad sequences to a fixed length\n",
        "    #encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "    encoded.insert(0,0)\n",
        "    encoded=array(encoded)\n",
        "    encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "    print(\"i:\", i, \"Encoded:\",encoded, encoded.shape)\n",
        "    # predict probability and sample a word from vocab\n",
        "    if i == 0:\n",
        "      prob = model.predict_proba(encoded, verbose=0)\n",
        "      print(\"i=\", i, \"Prob:\", prob, type(prob), prob.shape)\n",
        "      yhat=0\n",
        "      while yhat == 0:\n",
        "        yhat=numpy.random.choice(range(vocab_size),p=prob.ravel())\n",
        "      yhat=[yhat]\n",
        "      yhat=array(yhat)\n",
        "      yhat=numpy.reshape(yhat,newshape=(1,-1))\n",
        "      print(\"i:\",i,\"If yhat:\", yhat, yhat.shape)\n",
        "    else:\n",
        "      yhat=numpy.append(yhat,0)\n",
        "      yhat=numpy.reshape(yhat,newshape=(1,-1))\n",
        "      while yhat[0,i] == 0:\n",
        "        yhat=model.predict_classes(encoded, verbose=0)\n",
        "      print(\"i:\", i, \"Else yhat:\", yhat, yhat.shape)\n",
        "\n",
        "\n",
        "    # map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat[0,i]:\n",
        "        out_word = word\n",
        "        #print(\"index:\", index, \"out_word:\", out_word)\n",
        "        break\n",
        "\n",
        "    # append to input\n",
        "    in_text = in_text + out_word + ' '\n",
        "  return in_text\n",
        "'''\n",
        "#print(tokenizer.word_index.items())\n",
        "\n",
        "'''print(\"------------------Sample Sequence without Seed---------------------------------\")\n",
        "print(sample_seq_wo_seed(model, tokenizer, 8, vocab_size))\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "'''\n",
        "\n",
        "def sample_all_seq_wo_seed(model, tokenizer, n_words, vocab_size): #all the words are sampled\n",
        "  encoded=list()\n",
        "  in_text = ''\n",
        "  # generate a fixed number of words = n_words\n",
        "  for i in range(n_words):\n",
        "    # encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0] # for words not in the vocab it returns []\n",
        "    print(\"i:\", i, \"Encoded:\",encoded)\n",
        "    # pre-pad sequences to a fixed length\n",
        "    #encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "    encoded.insert(0,0)\n",
        "    encoded=array(encoded)\n",
        "    encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "    print(\"i:\", i, \"Encoded:\",encoded, encoded.shape)\n",
        "    # predict probability and sample a word from vocab\n",
        "    if i == 0:\n",
        "      prob = model.predict_proba(encoded)\n",
        "      print(\"i=\", i, \"Prob:\", prob, type(prob), prob.shape)\n",
        "      yhat=0\n",
        "      while yhat == 0:\n",
        "        yhat=numpy.random.choice(range(vocab_size),p=prob.ravel())\n",
        "      yhat=[yhat]\n",
        "      yhat=array(yhat)\n",
        "      yhat=numpy.reshape(yhat,newshape=(1,-1))\n",
        "      print(\"i:\", i, \"If yhat:\", yhat, yhat.shape)\n",
        "    else:\n",
        "      prob = model.predict_proba(encoded)\n",
        "      print(\"i=\", i, \"Prob:\", prob, type(prob), prob.shape)\n",
        "      yhat=numpy.append(yhat,0)  #just creating space for the next yhat\n",
        "      yhat=numpy.reshape(yhat,newshape=(1,-1))\n",
        "      while yhat[0,i] == 0:\n",
        "        yhat[0,i]=numpy.random.choice(range(vocab_size),p=prob[0,i].ravel())\n",
        "      print(\"i:\", i, \"Else yhat:\", yhat, yhat.shape)\n",
        "\n",
        "\n",
        "    # map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat[0,i]:\n",
        "        out_word = word\n",
        "        #print(\"index:\", index, \"out_word:\", out_word)\n",
        "        break\n",
        "\n",
        "    # append to input\n",
        "    in_text = in_text + out_word + ' '\n",
        "  return in_text\n",
        "\n",
        "print(\"-----------------------sample all seq without seed-----------------------------\")\n",
        "print(sample_all_seq_wo_seed(model, tokenizer, 8, vocab_size))\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "def prob_of_input_sentence(model, tokenizer, sentence):\n",
        "  print(\"Input Sentence:\", sentence)\n",
        "  encoded=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  print(\"encoded before insert:\", encoded)\n",
        "  encoded.insert(0,0)\n",
        "  encoded=array(encoded)\n",
        "  encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "  print(\"Encoded:\", encoded, encoded.shape)\n",
        "  prob=model.predict_proba(encoded, verbose=0)\n",
        "  print(\"Prob:\", prob, prob.shape)\n",
        "  probability=1\n",
        "  for i in range(prob.shape[1]-1):\n",
        "    probability = probability * prob[0,i,encoded[0,i+1]]\n",
        "  print(\"Probability of Sentence\", \"\\\"\", sentence, \"\\\"\", \"is:\", probability)\n",
        "\n",
        "print(\"-------------------Probability of Input Sentence------------------------------\")\n",
        "prob_of_input_sentence(model, tokenizer, \"Jack and Jill Went\")\n",
        "prob_of_input_sentence(model, tokenizer, \"and Jill Went up\")\n",
        "prob_of_input_sentence(model, tokenizer, \"went up the hill\")\n",
        "prob_of_input_sentence(model, tokenizer, \"jack and Jill Went up the hill .\")\n",
        "prob_of_input_sentence(model, tokenizer, \"to fetch a pail\")\n",
        "prob_of_input_sentence(model, tokenizer, \"fetch a pail\")\n",
        "prob_of_input_sentence(model, tokenizer, \"to fetch a pail of water\")\n",
        "prob_of_input_sentence(model, tokenizer, \"jack fell down and\")\n",
        "prob_of_input_sentence(model, tokenizer, \"jack fell down and broke\")\n",
        "prob_of_input_sentence(model, tokenizer, \"fell down and broke\")\n",
        "prob_of_input_sentence(model, tokenizer, \"jack fell down and broke his crown\")\n",
        "prob_of_input_sentence(model, tokenizer, \"and jill came tumbling\")\n",
        "prob_of_input_sentence(model, tokenizer, \"jill came tumbling after\")\n",
        "prob_of_input_sentence(model, tokenizer, \"and jill came tumbling after .\")\n",
        "print(\"-------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DeG8wjTN8jhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}